{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14926842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Oracle_Assignment_2 as oa\n",
    "# import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# oa.q2_get_mnist_jpg_subset(23634)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c3da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2c6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b085d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"q2_data\", transform=transform)\n",
    "\n",
    "# Split dataset into training and test sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6615895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad() :\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs, test_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights using SGD with momentum\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # evaluate(model, train_loader, test_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "def metricslearn(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.numpy().reshape(images.shape[0], -1)\n",
    "        data_list.append(images)\n",
    "        labels_list.append(labels.numpy())\n",
    "\n",
    "    data_array = np.vstack(data_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    return data_array, labels_array\n",
    "\n",
    "train_data, train_labels = dataloader_to_numpy(train_loader)\n",
    "test_data, test_labels = dataloader_to_numpy(test_loader)\n",
    "\n",
    "\n",
    "def pca(data, k) :\n",
    "    cov_matrix = np.cov(data.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvecks = eigenvectors[:, :k]\n",
    "    new_data = np.dot(data, eigenvecks)\n",
    "    return new_data, eigenvecks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10463a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7390315429f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mean = np.mean(train_data, axis=0)\n",
    "std = np.std(train_data, axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "k = 100\n",
    "train_data_pca, eigenvecks = pca(train_data, k)\n",
    "test_data_pca = np.dot(test_data, eigenvecks)\n",
    "\n",
    "\n",
    "\n",
    "# Convert PCA-transformed data to tensors\n",
    "train_data_pca_tensor = torch.tensor(train_data_pca, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data_pca_tensor = torch.tensor(test_data_pca, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_pca_dataset = TensorDataset(train_data_pca_tensor, train_labels_tensor)\n",
    "test_pca_dataset = TensorDataset(test_data_pca_tensor, test_labels_tensor)\n",
    "\n",
    "train_pca_loader = DataLoader(train_pca_dataset, batch_size=32, shuffle=True)\n",
    "test_pca_loader = DataLoader(test_pca_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Reconstruct a sample image\n",
    "sample = train_data_pca[10]\n",
    "reconstructed = np.dot(sample, eigenvecks.T)\n",
    "reconstructed = (reconstructed * std) + mean\n",
    "reconstructed = reconstructed.reshape(28, 28)\n",
    "plt.imshow(reconstructed, cmap=\"gray\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01f0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = KAN(width=[100,75,35,10],grid =5 ,k=3,seed=42,device=device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd45a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ -1.1363, -10.1142,   3.1149,  -7.3692,  -4.9781,  -0.3431,   0.1415,\n",
      "          0.2161,  -1.8036,  -2.6631,   1.9685,   0.9041,  -0.5736,   2.7548,\n",
      "         -2.4869,   1.8764,  -0.1817,   2.6062,  -1.4688,   4.2357,   0.2727,\n",
      "          2.8406,  -0.0665,  -3.8878,  -0.3576,   3.5008,   0.4086,   0.8712,\n",
      "          1.2113,  -0.2676,  -0.9633,  -0.4784,   0.1877,  -0.4097,  -0.6399,\n",
      "          1.9619,  -0.1484,  -0.2348,  -2.1437,  -1.6351,   1.0483,   0.6569,\n",
      "         -1.2990,  -0.9371,  -1.3625,  -1.9442,  -0.7431,  -0.3556,   1.2343,\n",
      "          0.4668,   1.1941,  -0.6166,  -0.3819,  -0.8315,   0.1817,   0.7214,\n",
      "          1.4191,   1.4045,  -0.6565,  -1.1309,  -1.6980,  -0.6644,   1.0827,\n",
      "         -2.1567,  -0.1445,   0.9523,   1.2380,   1.5561,   0.8930,  -1.2713,\n",
      "         -2.8462,  -0.4108,   1.4259,   1.6058,   1.4215,   3.0007,  -2.4232,\n",
      "         -0.6198,   1.3759,  -0.6399,  -2.2057,   0.0129,   0.7368,   0.2263,\n",
      "         -0.1825,   0.6191,   0.0520,  -0.5391,   0.3354,   0.7886,   0.7323,\n",
      "         -1.2501,  -0.5519,  -0.7940,   0.3693,   1.7464,  -0.4714,  -0.3138,\n",
      "          0.3465,   0.8771]), tensor(3))\n",
      "Epoch [1/20], Loss: 1.1790\n",
      "Epoch [2/20], Loss: 0.5101\n",
      "Epoch [3/20], Loss: 0.3863\n",
      "Epoch [4/20], Loss: 0.3316\n",
      "Epoch [5/20], Loss: 0.2886\n",
      "Epoch [6/20], Loss: 0.2389\n",
      "Epoch [7/20], Loss: 0.2086\n",
      "Epoch [8/20], Loss: 0.1789\n",
      "Epoch [9/20], Loss: 0.1537\n",
      "Epoch [10/20], Loss: 0.1281\n",
      "Epoch [11/20], Loss: 0.1088\n",
      "Epoch [12/20], Loss: 0.1011\n",
      "Epoch [13/20], Loss: 0.0864\n",
      "Epoch [14/20], Loss: 0.0727\n",
      "Epoch [15/20], Loss: 0.0659\n",
      "Epoch [16/20], Loss: 0.0542\n",
      "Epoch [17/20], Loss: 0.0976\n",
      "Epoch [18/20], Loss: 0.0866\n",
      "Epoch [19/20], Loss: 0.0601\n",
      "Epoch [20/20], Loss: 0.0444\n",
      "Test Accuracy: 90.85%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len(train_pca_dataset)\n",
    "for i in train_pca_dataset:\n",
    "    print (i)\n",
    "    break\n",
    "\n",
    "\n",
    "# train(model,train_data_pca)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model, train_pca_loader, criterion, optimizer, device, epochs=20, test_loader = test_pca_loader)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc735a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.85%\n",
      "Train Accuracy: 98.96%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad() :\n",
    "    for images, labels in train_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a78f06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for kan model = 146650\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "      p_count = p.numel()\n",
    "      count += p_count\n",
    "\n",
    "print(\"Number of parameters for kan model =\", count)\n",
    "# print(\"The time taken to train for kan model =\", end1-start1)\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for p in mlp_model.parameters():\n",
    "#     if p.requires_grad:\n",
    "#       p_count = p.numel()\n",
    "#       count += p_count\n",
    "\n",
    "# print(\"Number of parameters for mlp model =\", count)\n",
    "# print(\"The time taken to train for mlp model =\", end2-start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60d0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
