{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14926842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Oracle_Assignment_2 as oa\n",
    "# import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# oa.q2_get_mnist_jpg_subset(23634)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c3da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2c6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b085d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"q2_data\", transform=transform)\n",
    "\n",
    "# Split dataset into training and test sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6615895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad() :\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs, test_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights using SGD with momentum\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # evaluate(model, train_loader, test_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "def metricslearn(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.numpy().reshape(images.shape[0], -1)\n",
    "        data_list.append(images)\n",
    "        labels_list.append(labels.numpy())\n",
    "\n",
    "    data_array = np.vstack(data_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    return data_array, labels_array\n",
    "\n",
    "train_data, train_labels = dataloader_to_numpy(train_loader)\n",
    "test_data, test_labels = dataloader_to_numpy(test_loader)\n",
    "\n",
    "\n",
    "def pca(data, k) :\n",
    "    cov_matrix = np.cov(data.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvecks = eigenvectors[:, :k]\n",
    "    new_data = np.dot(data, eigenvecks)\n",
    "    return new_data, eigenvecks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10463a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x78bb9a92d280>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mean = np.mean(train_data, axis=0)\n",
    "std = np.std(train_data, axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "k = 100\n",
    "train_data_pca, eigenvecks = pca(train_data, k)\n",
    "test_data_pca = np.dot(test_data, eigenvecks)\n",
    "\n",
    "\n",
    "\n",
    "# Convert PCA-transformed data to tensors\n",
    "train_data_pca_tensor = torch.tensor(train_data_pca, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data_pca_tensor = torch.tensor(test_data_pca, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_pca_dataset = TensorDataset(train_data_pca_tensor, train_labels_tensor)\n",
    "test_pca_dataset = TensorDataset(test_data_pca_tensor, test_labels_tensor)\n",
    "\n",
    "train_pca_loader = DataLoader(train_pca_dataset, batch_size=32, shuffle=True)\n",
    "test_pca_loader = DataLoader(test_pca_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Reconstruct a sample image\n",
    "sample = train_data_pca[10]\n",
    "reconstructed = np.dot(sample, eigenvecks.T)\n",
    "reconstructed = (reconstructed * std) + mean\n",
    "reconstructed = reconstructed.reshape(28, 28)\n",
    "plt.imshow(reconstructed, cmap=\"gray\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01f0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = KAN(width=[100,10,20,10],grid =5 ,k=3,seed=42,device=device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd45a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-7.7189e+00,  1.4777e+00,  9.4306e+00,  8.8542e+00,  7.2840e+00,\n",
      "        -4.1538e-01,  7.0594e+00, -1.4637e+00, -8.2414e+00,  6.1388e+00,\n",
      "         1.9317e+00, -1.1389e+01,  1.5030e+00,  4.9363e-01, -5.3901e+00,\n",
      "        -1.9492e+00,  1.2651e+00, -4.7074e+00,  5.9792e+00,  3.5035e+00,\n",
      "        -2.7716e+00,  4.5507e+00,  9.5497e-01, -2.2675e-01, -5.4888e+00,\n",
      "         4.1360e+00,  5.5034e+00,  1.4171e+00,  1.3097e+00,  3.8884e+00,\n",
      "         4.5593e-01, -3.8053e+00, -3.9672e+00,  1.1866e+00,  4.9105e+00,\n",
      "        -4.1893e+00, -1.2401e+00, -1.6087e+00, -5.3492e+00,  1.9906e+00,\n",
      "        -4.8786e-01,  3.1837e+00,  1.1575e+00,  3.9796e+00, -1.0394e+00,\n",
      "         9.3361e-01,  1.3173e+00, -2.5063e+00,  5.1016e-01, -1.4527e+00,\n",
      "        -3.8319e+00, -7.1910e-01, -1.3766e+00, -1.2369e+00,  5.3003e-01,\n",
      "        -1.3384e+00,  5.0514e-03,  5.6978e-01,  7.0993e-02,  1.6389e+00,\n",
      "         3.4770e-01,  2.9714e+00, -1.1256e+00, -3.2796e+00, -5.0117e-01,\n",
      "         2.4837e+00, -9.0936e-01,  1.0304e+00, -3.6438e+00, -9.1685e-02,\n",
      "         1.4518e+00, -2.1458e-01,  1.8461e+00, -9.8732e-01,  4.7954e-01,\n",
      "        -2.0628e+00,  3.0767e+00,  3.5792e-01,  2.2950e+00,  1.0098e+00,\n",
      "        -3.0236e+00, -2.4693e-01, -1.4908e-01, -1.7613e+00, -2.4969e+00,\n",
      "        -3.1092e-01,  2.0130e-01,  7.4668e-01,  9.1948e-01,  1.5251e+00,\n",
      "         1.5820e+00, -7.8299e-01,  2.1213e+00, -1.8825e-01,  1.3857e+00,\n",
      "        -2.4126e+00, -3.1194e+00,  3.0497e-01, -1.8349e+00, -4.3697e+00]), tensor(2))\n",
      "Epoch [1/17], Loss: 1.3753\n",
      "Epoch [2/17], Loss: 0.6200\n",
      "Epoch [3/17], Loss: 0.5230\n",
      "Epoch [4/17], Loss: 0.5005\n",
      "Epoch [5/17], Loss: 0.4530\n",
      "Epoch [6/17], Loss: 0.4325\n",
      "Epoch [7/17], Loss: 0.4064\n",
      "Epoch [8/17], Loss: 0.4030\n",
      "Epoch [9/17], Loss: 0.3871\n",
      "Epoch [10/17], Loss: 0.3683\n",
      "Epoch [11/17], Loss: 0.3694\n",
      "Epoch [12/17], Loss: 0.3806\n",
      "Epoch [13/17], Loss: 0.3672\n",
      "Epoch [14/17], Loss: 0.3461\n",
      "Epoch [15/17], Loss: 0.3591\n",
      "Epoch [16/17], Loss: 0.3416\n",
      "Epoch [17/17], Loss: 0.3540\n",
      "Test Accuracy: 85.65%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len(train_pca_dataset)\n",
    "for i in train_pca_dataset:\n",
    "    print (i)\n",
    "    break\n",
    "\n",
    "\n",
    "# train(model,train_data_pca)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model, train_pca_loader, criterion, optimizer, device, epochs=17, test_loader = test_pca_loader)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc735a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.65%\n",
      "Train Accuracy: 90.44%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad() :\n",
    "    for images, labels in train_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ccaa0f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LBFGS.step() missing 1 required positional argument: 'closure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     25\u001b[39m optimizer = optim.LBFGS(modelMLPPCA.parameters(), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelMLPPCA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pca_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pca_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m metricslearn(modelMLPPCA, train_pca_loader, test_pca_loader, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epochs, test_loader)\u001b[39m\n\u001b[32m     32\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     33\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update weights using SGD with momentum\u001b[39;00m\n\u001b[32m     36\u001b[39m     total_loss += loss.item()\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# evaluate(model, train_loader, test_loader, device)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/eminst/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/eminst/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: LBFGS.step() missing 1 required positional argument: 'closure'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MLPPCA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPPCA, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "modelMLPPCA = MLPPCA()\n",
    "\n",
    "\n",
    "modelMLPPCA = MLPPCA().to(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.LBFGS(modelMLPPCA.parameters(), lr=0.01)\n",
    "train(modelMLPPCA, train_pca_loader, criterion, optimizer, \"cpu\", epochs=17, test_loader = test_pca_loader)\n",
    "metricslearn(modelMLPPCA, train_pca_loader, test_pca_loader, \"cpu\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f06ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
