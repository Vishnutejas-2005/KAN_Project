{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14926842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Oracle_Assignment_2 as oa\n",
    "# import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# oa.q2_get_mnist_jpg_subset(23634)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c3da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2c6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b085d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"q2_data\", transform=transform)\n",
    "\n",
    "# Split dataset into training and test sets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6615895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad() :\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs, test_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights using SGD with momentum\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # evaluate(model, train_loader, test_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "def metricslearn(model, train_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.numpy().reshape(images.shape[0], -1)\n",
    "        data_list.append(images)\n",
    "        labels_list.append(labels.numpy())\n",
    "\n",
    "    data_array = np.vstack(data_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    return data_array, labels_array\n",
    "\n",
    "train_data, train_labels = dataloader_to_numpy(train_loader)\n",
    "test_data, test_labels = dataloader_to_numpy(test_loader)\n",
    "\n",
    "\n",
    "def pca(data, k) :\n",
    "    cov_matrix = np.cov(data.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvecks = eigenvectors[:, :k]\n",
    "    new_data = np.dot(data, eigenvecks)\n",
    "    return new_data, eigenvecks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10463a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7398c9e60770>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIgZJREFUeJzt3Xts1fX9x/HXaWkPhbanK5depECLKE6giww6oiKOBugSI0oWb3+AMRhZMUPmNCwqui3phokzLgz/2WAm4i0RiGZjEZASN2ABZcRsdhSL4HqDzl4ovffz+4PQ/Y5c5PPhnO/n9PB8JCeBc86753O+53vOi0O/53VCxhgjAAACluJ7AQCAaxMBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMCLEb4X8HWDg4Oqr69XVlaWQqGQ7+UAACwZY9TR0aHCwkKlpFz6fU7CBVB9fb2Kiop8LwMAcJVOnjypCRMmXPLyhAugrKwsSdLkyZMvm5zDjcu7OZeWpCDfNbrcVlDbwVVQ63OZcXk+DAwMWM9IUlpaWiC3NTg4aD3jsjaX25Hc7lNqamogt5PIBgcHdfz48aHX80uJWwBt2LBBL774ohobG1VaWqrf/va3mjNnzjfOnX8BSElJSaoAcrkvLk+aILcZAXROIgeQ67ZzeRENisvaXP9h5rL9XNaXrJWc37Td4/Jq9dZbb2nNmjVat26dPv74Y5WWlmrRokVqbm6Ox80BAIahuATQSy+9pBUrVujhhx/Wt7/9bb366qsaNWqU/vCHP8Tj5gAAw1DMA6i3t1eHDh1SeXn5/24kJUXl5eXat2/fBdfv6elRe3t71AkAkPxiHkCnT5/WwMCA8vLyos7Py8tTY2PjBdevqqpSJBIZOnEEHABcG7z/ln/t2rVqa2sbOp08edL3kgAAAYj5UXBjx45Vamqqmpqaos5vampSfn7+BdcPh8MKh8OxXgYAIMHF/B1Qenq6Zs2apV27dg2dNzg4qF27dmnu3LmxvjkAwDAVl88BrVmzRsuWLdN3v/tdzZkzRy+//LI6Ozv18MMPx+PmAADDUFwC6L777tOpU6f03HPPqbGxUd/5zne0Y8eOCw5MAABcu0ImwT6C297erkgkopKSEqtPfgfZABBUQ4FrfUhQgtrmiV5K67K+oOpaXB+j/v5+65mgmjFcnheu+9CIEfb/Ru/r67OeSfTnuq3BwUF9/vnnamtrU3Z29iWv5/0oOADAtYkAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXsSlDduHRC/zS8YCU5eCR5fu2yBLOBN9P7Llen9ctl9Q5bQu98llH5ISvwh3uOMdEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALxI2DbsUChk1a6bbC3GUuLfJ5em4NTUVOuZILeDS6OzS8N3f3+/9cyIEfZP15EjR1rPSFI4HLaecblPPT091jNpaWnWM65t2C6PbaK32CcS3gEBALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcJW0ZqjIl7QZ9LaaCroMoGXQorXbkUVrqUO7psO5diTNc5l22emZmZsDOS2zZva2uznunu7raecdmHgnxeuHC5T64Fqy5sXyuv9P7wDggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvEjshr44C6og1FVqaqr1jEvBaigUsp6R3MoQgyqAdb0dl9LKSCRiPVNcXGw9M3nyZOuZ7Oxs6xlJOnXqlPXM0aNHrWc6OzutZ7q6uqxn0tPTrWckt8Ldvr4+65kgn0sur3u2M5SRAgASGgEEAPAi5gH0/PPPKxQKRZ2mTZsW65sBAAxzcfkd0M0336ydO3f+70YS/MugAADBi0syjBgxQvn5+fH40QCAJBGX3wEdPXpUhYWFKikp0UMPPaQTJ05c8ro9PT1qb2+POgEAkl/MA6isrEybN2/Wjh07tHHjRtXV1en2229XR0fHRa9fVVWlSCQydCoqKor1kgAACSjmAVRRUaEf/vCHmjlzphYtWqQ//elPam1t1dtvv33R669du1ZtbW1Dp5MnT8Z6SQCABBT3owNycnJ0ww03qLa29qKXh8Nhpw97AQCGt7h/DujMmTM6duyYCgoK4n1TAIBhJOYB9OSTT6q6ulrHjx/X3/72N91zzz1KTU3VAw88EOubAgAMYzH/L7gvv/xSDzzwgFpaWjRu3Djddttt2r9/v8aNGxfrmwIADGMxD6A333wz1j/yirgU8yV6GakLl1JDlxnJ7QPGI0eOtJ5JS0uznnHlUszqUhI6Z84c65np06dbz7gU2krSZ599Zj1z/Phx65lLHR17OS77q2sZqcv+EETZ59VIpNdKuuAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwIu4fyFdUFzK8lxK+Vy5lBoaY6xngiwjdSkWzczMtJ5x2Q5fffWV9YzkVnyakZFhPTNmzBjrGZfv1HIpjJWk+vp66xmXx6mrq8t6xoXrdnDZx4MqI3V5TZHcHqd44R0QAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvEiaNmwXrm2yicyl2TrIpuBwOGw909LSYj3T0NBgPSNJWVlZ1jNBNhkHdTsuj60Ll/111KhRgcxIbs+N9PR065ne3l7rmf7+fuuZRMM7IACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwImHLSEOhkFJSrjwfXQohXbkUPLoUBxpjrGdcuJQnSm6Fld3d3dYzjY2N1jMdHR3WM5I0adIk65lp06ZZzxQXF1vP5OTkWM+4cnlsU1NTrWfS0tICuR3Xwl2X15WgnutBvT5I8SvP5R0QAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHiRsGWkxpi4F4y6lvkFVRzoMuNS1OhaRury+DQ3N1vPtLe3W89MmDDBekaSFixYYD1z5513Ws+UlJRYz7g8Ti7bTpJ6e3utZ1wKTEePHm09E1QZsORWnhtUGWm8CkKDxDsgAIAXBBAAwAvrANq7d6/uuusuFRYWKhQKadu2bVGXG2P03HPPqaCgQBkZGSovL9fRo0djtV4AQJKwDqDOzk6VlpZqw4YNF718/fr1euWVV/Tqq6/qwIEDGj16tBYtWuT0f6kAgORlfRBCRUWFKioqLnqZMUYvv/yynnnmGd19992SpNdee015eXnatm2b7r///qtbLQAgacT0d0B1dXVqbGxUeXn50HmRSERlZWXat2/fRWd6enrU3t4edQIAJL+YBlBjY6MkKS8vL+r8vLy8ocu+rqqqSpFIZOhUVFQUyyUBABKU96Pg1q5dq7a2tqHTyZMnfS8JABCAmAZQfn6+JKmpqSnq/KampqHLvi4cDis7OzvqBABIfjENoOLiYuXn52vXrl1D57W3t+vAgQOaO3duLG8KADDMWR8Fd+bMGdXW1g79va6uTocPH1Zubq4mTpyo1atX65e//KWmTp2q4uJiPfvssyosLNSSJUtiuW4AwDBnHUAHDx6M6r5as2aNJGnZsmXavHmznnrqKXV2durRRx9Va2urbrvtNu3YscOpJwoAkLxCxrWRM07a29sViURUUlKilJT4HiPh+vPjXZJ6nstDk5aWZj3j+o+DM2fOWM+0tLRYz4TDYeuZO+64w3pGktNn1crKyqxnBgYGrGdctrfLjCT94x//sJ7ZuXOn9czhw4etZ1yef67PdZdSVpeSUJeZeL8+Xo2BgQEdO3ZMbW1tl/29fuLeAwBAUiOAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMAL669jCEooFLJqe3VpyA2q1VqSUlNTrWdcGnJdbse1Mfmrr76ynnFp8C0pKbGemTp1qvWM5NYMXldXZz3T3t5uPeOiv7/faa6rqyvGK4mdzs5O6xmX55IkjRhh/xLpMuPC9T4l0hcg8A4IAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALxI2DJSY0ygZaHx5lIc6FLc6VI+2dHRYT3j6rrrrrOecSkWzczMtJ6RpOPHj1vP9PX1Wc/09PRYz+Tk5FjPpKenW89IbutzKTB12fdcZrKysqxnJLdiUZfn7cDAgPVMkGxfi6+08JR3QAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgRcKWkdpyKQAMsuzUpdSwt7fXesalRNKlKFWSCgoKrGduueUW65mbbrrJesa1jNSlULO7u9t6xqUcMzc313omIyPDekaSWltbrWc6OzutZ1yKRV32V9ftMGrUKOsZl+ety0xqaqr1jOT+fI8H3gEBALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBdJU0YaZLGoC2OM9Ux/f38cVnKhnJwcp7nS0lLrmTvvvNN6ZsaMGdYzaWlp1jOS1NzcbD3j8ji5lJG6lL+67kOnTp1ymrPlUozpUjTrWk4bDoetZ1yKRV1eH1xf81yKm21nrvT+8A4IAOAFAQQA8MI6gPbu3au77rpLhYWFCoVC2rZtW9Tly5cvVygUijotXrw4VusFACQJ6wDq7OxUaWmpNmzYcMnrLF68WA0NDUOnN95446oWCQBIPtYHIVRUVKiiouKy1wmHw8rPz3deFAAg+cXld0B79uzR+PHjdeONN2rlypVqaWm55HV7enrU3t4edQIAJL+YB9DixYv12muvadeuXfr1r3+t6upqVVRUaGBg4KLXr6qqUiQSGToVFRXFekkAgAQU888B3X///UN/njFjhmbOnKkpU6Zoz549WrBgwQXXX7t2rdasWTP09/b2dkIIAK4BcT8Mu6SkRGPHjlVtbe1FLw+Hw8rOzo46AQCSX9wD6Msvv1RLS4vTp7gBAMnL+r/gzpw5E/Vupq6uTocPH1Zubq5yc3P1wgsvaOnSpcrPz9exY8f01FNP6frrr9eiRYtiunAAwPBmHUAHDx6M6vM6//ubZcuWaePGjTpy5Ij++Mc/qrW1VYWFhVq4cKF+8YtfOHUqAQCSl3UAzZ8//7JFc3/5y1+uakHnhUIhqwI8l1JDlwJAV0GVDWZkZFjPFBcXW89I0qxZs6xnZs+ebT1z0003Wc/09fVZz0jSiBH2x+X09PRYz7iUYwZ5cE5dXZ31jMu+N3LkSOsZlyJX1zJSl+etS9lnkK9FLmxfiygjBQAkNAIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALyI+Vdyx0ooFLJquA6yTdal7ba/vz8OK7mQS+tvfn6+022NGzfOac5WfX299cyJEyecbuvf//639czAwID1jEuzdV5envXMqFGjrGckqbu723qmt7fXeiY1NdV6xqWx3OU5K7ndJ5f9wXV9QbFdH23YAICERgABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvEraM1BgT94JRm7LT/89lXYODg9YzLgWFLuWOrkWIX331lfXMwYMHA7kdl1JRSWpsbLSeGT16tPXM7NmzrWcmT55sPeNaRvrFF19Yz5w+fdp65uzZs9Yz2dnZ1jMupaKSWymrSxlpkM9bF7avX5SRAgASGgEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8SNgyUluuxaKJfFsut9PX12c9c+bMGesZSaqtrbWe6ezstJ45fvx4IDOS1N/fbz0zbtw46xmXQs0bbrghkNuRpPr6eusZl8fWhctj5FJ6KrmVmLqUhAb5+uVSpmx7nygjBQAkNAIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4kTRlpC5cSvkkt+LAESPsN7VL6aJLeWJLS4v1jORWYtrU1GQ988UXX1jPnDp1ynrGlcs2P336tPVMc3Oz9YzLPiRJXV1d1jMu+3haWpr1TJBlpC6vEampqdYzLq8prq9fLgYHB62uTxkpACChEUAAAC+sAqiqqkqzZ89WVlaWxo8fryVLlqimpibqOt3d3aqsrNSYMWOUmZmppUuXOv23CwAguVkFUHV1tSorK7V//3598MEH6uvr08KFC6O+iOqJJ57Qe++9p3feeUfV1dWqr6/XvffeG/OFAwCGN6vfGu7YsSPq75s3b9b48eN16NAhzZs3T21tbfr973+vLVu26Pvf/74kadOmTbrpppu0f/9+fe9734vdygEAw9pV/Q6ora1NkpSbmytJOnTokPr6+lReXj50nWnTpmnixInat2/fRX9GT0+P2tvbo04AgOTnHECDg4NavXq1br31Vk2fPl2S1NjYqPT0dOXk5ERdNy8vT42NjRf9OVVVVYpEIkOnoqIi1yUBAIYR5wCqrKzUp59+qjfffPOqFrB27Vq1tbUNnU6ePHlVPw8AMDw4fRB11apVev/997V3715NmDBh6Pz8/Hz19vaqtbU16l1QU1OT8vPzL/qzwuGwwuGwyzIAAMOY1TsgY4xWrVqlrVu3avfu3SouLo66fNasWUpLS9OuXbuGzqupqdGJEyc0d+7c2KwYAJAUrN4BVVZWasuWLdq+fbuysrKGfq8TiUSUkZGhSCSiRx55RGvWrFFubq6ys7P1+OOPa+7cuRwBBwCIYhVAGzdulCTNnz8/6vxNmzZp+fLlkqTf/OY3SklJ0dKlS9XT06NFixbpd7/7XUwWCwBIHlYBdCUFcyNHjtSGDRu0YcMG50VJ58r5bAr6gizmC0pKiv0xIralgZJbMabkVgrpcpi9S+mpq/T0dOsZl99hZmZmWs9kZWVZz7jcH9c5l/vk8ti6lH26PC8kt5JQF67rS/Tb+iZ0wQEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALp29EDYIxJikbrm0E1Ybd1dVlPSO5tWG7rC8jI8N6Jjs723pGOvfdVra+/sWMV6KkpMR65lLfKnw5PT091jOSW+O0Syu4S+v2wMCA9cyIEW4vdS5t2C4zQTZUB/W6ciV4BwQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXiRsGaktlwJA17JTlzmX9QUlyLW5lEJmZmZaz6SlpVnPSG7lmEGVcJ49e9Z6prW11XpGkjo7O61nXEpCXR4nl33IpYBTcivhDLJY1EUirY93QAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgRdKUkboUIboWFLqUkbrMuJQGutwn1zJSl1JIlxkXLvuDJHV3d1vPNDY2Ws8cPXrUeiaoAlNJ+u9//2s909fX53RbtoIszw2q5DiRCkKDxDsgAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPAiYctIjTEJW9AX1Lpcy1JtuZQnSm7bweW2XIpFe3t7rWckqaenx3rGpcB09OjRgdxOf3+/9Ywk/ec//7Ge6ejosJ5x2YdSU1OtZ1wFWXx6LeIdEADACwIIAOCFVQBVVVVp9uzZysrK0vjx47VkyRLV1NREXWf+/PkKhUJRp8ceeyymiwYADH9WAVRdXa3Kykrt379fH3zwgfr6+rRw4UJ1dnZGXW/FihVqaGgYOq1fvz6miwYADH9WByHs2LEj6u+bN2/W+PHjdejQIc2bN2/o/FGjRik/Pz82KwQAJKWr+h1QW1ubJCk3Nzfq/Ndff11jx47V9OnTtXbt2st+LXBPT4/a29ujTgCA5Od8GPbg4KBWr16tW2+9VdOnTx86/8EHH9SkSZNUWFioI0eO6Omnn1ZNTY3efffdi/6cqqoqvfDCC67LAAAMUyHj+CGQlStX6s9//rM++ugjTZgw4ZLX2717txYsWKDa2lpNmTLlgst7enqiPnvR3t6uoqIilZSUxP1zMK4/P9k+B+SKzwGdk5GRYT0zdepU65mJEydazwT5OaDm5mbrmaA+BxTk53n6+vqsZ1z28UQ2ODiozz//XG1tbcrOzr7k9ZzeAa1atUrvv/++9u7de9nwkaSysjJJumQAhcNhhcNhl2UAAIYxqwAyxujxxx/X1q1btWfPHhUXF3/jzOHDhyVJBQUFTgsEACQnqwCqrKzUli1btH37dmVlZamxsVGSFIlElJGRoWPHjmnLli36wQ9+oDFjxujIkSN64oknNG/ePM2cOTMudwAAMDxZBdDGjRslnfuw6f+3adMmLV++XOnp6dq5c6defvlldXZ2qqioSEuXLtUzzzwTswUDAJKD9X/BXU5RUZGqq6uvakEAgGtDwrZhh0Ihq6PAErU5O2hBbocRI+x3H5cZF11dXU5zLtvPZaahocF65nKfp7sU16bzr7ebXAmXIw9djmhzOWLM9YhSl6PnaNC+col9nC8AIGkRQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwIuELSM1xsS9WNP15wf1VdmJXrDqsj6Xr4h22d5paWnWM5KUmZnpNGfLZTu0tLRYzwT5tfOJ/FXZQT7XXQtgr0W8AwIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4kXBfc+R6lRO5BC6rrKdE7pVzWF9SM6/6TyPudC9d9KKh9L6guOFcu22FgYMB6Jtn2u/P355u2X8IFUEdHhyTp+PHjfhcCALgqHR0dikQil7w8ZBLsn9mDg4Oqr69XVlbWBf86am9vV1FRkU6ePKns7GxPK/SP7XAO2+EctsM5bIdzEmE7GGPU0dGhwsLCyzaKJ9w7oJSUFE2YMOGy18nOzr6md7Dz2A7nsB3OYTucw3Y4x/d2uNw7n/M4CAEA4AUBBADwYlgFUDgc1rp16xQOh30vxSu2wzlsh3PYDuewHc4ZTtsh4Q5CAABcG4bVOyAAQPIggAAAXhBAAAAvCCAAgBfDJoA2bNigyZMna+TIkSorK9Pf//5330sK3PPPP69QKBR1mjZtmu9lxd3evXt11113qbCwUKFQSNu2bYu63Bij5557TgUFBcrIyFB5ebmOHj3qZ7Fx9E3bYfny5RfsH4sXL/az2DipqqrS7NmzlZWVpfHjx2vJkiWqqamJuk53d7cqKys1ZswYZWZmaunSpWpqavK04vi4ku0wf/78C/aHxx57zNOKL25YBNBbb72lNWvWaN26dfr4449VWlqqRYsWqbm52ffSAnfzzTeroaFh6PTRRx/5XlLcdXZ2qrS0VBs2bLjo5evXr9crr7yiV199VQcOHNDo0aO1aNEidXd3B7zS+Pqm7SBJixcvjto/3njjjQBXGH/V1dWqrKzU/v379cEHH6ivr08LFy5UZ2fn0HWeeOIJvffee3rnnXdUXV2t+vp63XvvvR5XHXtXsh0kacWKFVH7w/r16z2t+BLMMDBnzhxTWVk59PeBgQFTWFhoqqqqPK4qeOvWrTOlpaW+l+GVJLN169ahvw8ODpr8/Hzz4osvDp3X2tpqwuGweeONNzysMBhf3w7GGLNs2TJz9913e1mPL83NzUaSqa6uNsace+zT0tLMO++8M3Sdf/3rX0aS2bdvn69lxt3Xt4Mxxtxxxx3mxz/+sb9FXYGEfwfU29urQ4cOqby8fOi8lJQUlZeXa9++fR5X5sfRo0dVWFiokpISPfTQQzpx4oTvJXlVV1enxsbGqP0jEomorKzsmtw/9uzZo/Hjx+vGG2/UypUr1dLS4ntJcdXW1iZJys3NlSQdOnRIfX19UfvDtGnTNHHixKTeH76+Hc57/fXXNXbsWE2fPl1r167V2bNnfSzvkhKujPTrTp8+rYGBAeXl5UWdn5eXp88++8zTqvwoKyvT5s2bdeONN6qhoUEvvPCCbr/9dn366afKysryvTwvGhsbJemi+8f5y64Vixcv1r333qvi4mIdO3ZMP/vZz1RRUaF9+/YpNTXV9/JibnBwUKtXr9att96q6dOnSzq3P6SnpysnJyfqusm8P1xsO0jSgw8+qEmTJqmwsFBHjhzR008/rZqaGr377rseVxst4QMI/1NRUTH055kzZ6qsrEyTJk3S22+/rUceecTjypAI7r///qE/z5gxQzNnztSUKVO0Z88eLViwwOPK4qOyslKffvrpNfF70Mu51HZ49NFHh/48Y8YMFRQUaMGCBTp27JimTJkS9DIvKuH/C27s2LFKTU294CiWpqYm5efne1pVYsjJydENN9yg2tpa30vx5vw+wP5xoZKSEo0dOzYp949Vq1bp/fff14cffhj19S35+fnq7e1Va2tr1PWTdX+41Ha4mLKyMklKqP0h4QMoPT1ds2bN0q5du4bOGxwc1K5duzR37lyPK/PvzJkzOnbsmAoKCnwvxZvi4mLl5+dH7R/t7e06cODANb9/fPnll2ppaUmq/cMYo1WrVmnr1q3avXu3iouLoy6fNWuW0tLSovaHmpoanThxIqn2h2/aDhdz+PBhSUqs/cH3URBX4s033zThcNhs3rzZ/POf/zSPPvqoycnJMY2Njb6XFqif/OQnZs+ePaaurs789a9/NeXl5Wbs2LGmubnZ99LiqqOjw3zyySfmk08+MZLMSy+9ZD755BPzxRdfGGOM+dWvfmVycnLM9u3bzZEjR8zdd99tiouLTVdXl+eVx9bltkNHR4d58sknzb59+0xdXZ3ZuXOnueWWW8zUqVNNd3e376XHzMqVK00kEjF79uwxDQ0NQ6ezZ88OXeexxx4zEydONLt37zYHDx40c+fONXPnzvW46tj7pu1QW1trfv7zn5uDBw+auro6s337dlNSUmLmzZvneeXRhkUAGWPMb3/7WzNx4kSTnp5u5syZY/bv3+97SYG77777TEFBgUlPTzfXXXedue+++0xtba3vZcXdhx9+aCRdcFq2bJkx5tyh2M8++6zJy8sz4XDYLFiwwNTU1PhddBxcbjucPXvWLFy40IwbN86kpaWZSZMmmRUrViTdP9Iudv8lmU2bNg1dp6ury/zoRz8y3/rWt8yoUaPMPffcYxoaGvwtOg6+aTucOHHCzJs3z+Tm5ppwOGyuv/5689Of/tS0tbX5XfjX8HUMAAAvEv53QACA5EQAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAAL/4PMjD7Kn3rgKEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mean = np.mean(train_data, axis=0)\n",
    "std = np.std(train_data, axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "k = 100\n",
    "train_data_pca, eigenvecks = pca(train_data, k)\n",
    "test_data_pca = np.dot(test_data, eigenvecks)\n",
    "\n",
    "\n",
    "\n",
    "# Convert PCA-transformed data to tensors\n",
    "train_data_pca_tensor = torch.tensor(train_data_pca, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_data_pca_tensor = torch.tensor(test_data_pca, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_pca_dataset = TensorDataset(train_data_pca_tensor, train_labels_tensor)\n",
    "test_pca_dataset = TensorDataset(test_data_pca_tensor, test_labels_tensor)\n",
    "\n",
    "train_pca_loader = DataLoader(train_pca_dataset, batch_size=32, shuffle=True)\n",
    "test_pca_loader = DataLoader(test_pca_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Reconstruct a sample image\n",
    "sample = train_data_pca[10]\n",
    "reconstructed = np.dot(sample, eigenvecks.T)\n",
    "reconstructed = (reconstructed * std) + mean\n",
    "reconstructed = reconstructed.reshape(28, 28)\n",
    "plt.imshow(reconstructed, cmap=\"gray\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01f0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from kan import *\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = KAN(width=[100,90,50,10],grid =5 ,k=3,seed=42,device=device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd45a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.2261e+01, -1.5074e+01, -3.1252e+00, -2.7496e+00, -1.5653e-02,\n",
      "         2.0129e+00, -5.0453e+00, -3.9234e-01,  8.4901e+00, -2.1361e+00,\n",
      "        -7.9032e-02,  1.4289e+00,  7.8916e-04, -2.5602e+00, -5.3239e+00,\n",
      "         1.3173e+00, -1.1822e+00,  1.8227e+00, -1.2008e+00, -3.8770e+00,\n",
      "        -3.4923e+00, -2.9638e+00, -2.1616e+00, -1.6173e+00,  1.4358e+00,\n",
      "         3.5759e+00, -1.8343e+00, -2.6306e+00,  8.2597e-01,  1.1188e+00,\n",
      "        -9.5460e-01, -2.2311e+00, -1.5846e+00,  1.3304e+00, -4.2420e-01,\n",
      "        -1.0424e+00,  1.3945e+00,  7.5510e-01,  1.9787e+00, -7.9653e-02,\n",
      "         5.6657e-01,  1.5060e+00,  1.2194e-01,  9.2942e-01, -7.7965e-01,\n",
      "        -1.1629e+00,  4.9531e-01,  1.4230e+00, -1.6863e+00,  7.9285e-01,\n",
      "        -6.8087e-02,  2.6648e+00, -1.3041e-01,  4.3609e-01,  1.6086e-02,\n",
      "        -1.5278e-01, -3.4654e-02, -1.8870e+00,  6.9497e-01,  1.4884e+00,\n",
      "        -4.2223e-01,  1.0946e+00,  3.4474e-01, -1.4096e-01, -4.7472e-01,\n",
      "        -4.3980e-01,  7.6985e-01, -1.0116e+00,  1.8425e-01,  9.1766e-01,\n",
      "         8.1578e-01,  2.6347e-01,  2.1708e-01,  1.2160e-01,  4.3979e-01,\n",
      "        -9.3030e-01, -1.0345e+00, -7.5898e-01,  8.9855e-02, -1.3356e+00,\n",
      "         6.7070e-01,  1.1290e+00,  3.1961e-01, -2.8323e-01, -2.1998e+00,\n",
      "         6.9401e-01,  5.0628e-01,  5.3277e-01, -2.3891e-01, -3.7352e-01,\n",
      "         1.5469e-01,  9.4588e-01,  2.0130e-02, -2.1416e+00, -1.1826e+00,\n",
      "         1.2913e+00, -5.5437e-01, -6.8616e-01, -7.1197e-01, -2.7360e+00]), tensor(0))\n",
      "Epoch [1/17], Loss: 1.2491\n",
      "Epoch [2/17], Loss: 0.5178\n",
      "Epoch [3/17], Loss: 0.3856\n",
      "Epoch [4/17], Loss: 0.3196\n",
      "Epoch [5/17], Loss: 0.2605\n",
      "Epoch [6/17], Loss: 0.2233\n",
      "Epoch [7/17], Loss: 0.1835\n",
      "Epoch [8/17], Loss: 0.1552\n",
      "Epoch [9/17], Loss: 0.1447\n",
      "Epoch [10/17], Loss: 0.1199\n",
      "Epoch [11/17], Loss: 0.1046\n",
      "Epoch [12/17], Loss: 0.0919\n",
      "Epoch [13/17], Loss: 0.0819\n",
      "Epoch [14/17], Loss: 0.0627\n",
      "Epoch [15/17], Loss: 0.0653\n",
      "Epoch [16/17], Loss: 0.0500\n",
      "Epoch [17/17], Loss: 0.0497\n",
      "Test Accuracy: 92.50%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len(train_pca_dataset)\n",
    "for i in train_pca_dataset:\n",
    "    print (i)\n",
    "    break\n",
    "\n",
    "\n",
    "# train(model,train_data_pca)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model, train_pca_loader, criterion, optimizer, device, epochs=17, test_loader = test_pca_loader)\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc735a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.50%\n",
      "Train Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad() :\n",
    "    for images, labels in train_pca_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Train Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ccaa0f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LBFGS.step() missing 1 required positional argument: 'closure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     25\u001b[39m optimizer = optim.LBFGS(modelMLPPCA.parameters(), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelMLPPCA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pca_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pca_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m metricslearn(modelMLPPCA, train_pca_loader, test_pca_loader, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epochs, test_loader)\u001b[39m\n\u001b[32m     32\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     33\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update weights using SGD with momentum\u001b[39;00m\n\u001b[32m     36\u001b[39m     total_loss += loss.item()\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# evaluate(model, train_loader, test_loader, device)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/eminst/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/eminst/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: LBFGS.step() missing 1 required positional argument: 'closure'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MLPPCA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPPCA, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(100, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "modelMLPPCA = MLPPCA()\n",
    "\n",
    "\n",
    "modelMLPPCA = MLPPCA().to(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.LBFGS(modelMLPPCA.parameters(), lr=0.01)\n",
    "train(modelMLPPCA, train_pca_loader, criterion, optimizer, \"cpu\", epochs=17, test_loader = test_pca_loader)\n",
    "metricslearn(modelMLPPCA, train_pca_loader, test_pca_loader, \"cpu\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78f06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for kan model = 196000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "      p_count = p.numel()\n",
    "      count += p_count\n",
    "\n",
    "print(\"Number of parameters for kan model =\", count)\n",
    "# print(\"The time taken to train for kan model =\", end1-start1)\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for p in mlp_model.parameters():\n",
    "#     if p.requires_grad:\n",
    "#       p_count = p.numel()\n",
    "#       count += p_count\n",
    "\n",
    "# print(\"Number of parameters for mlp model =\", count)\n",
    "# print(\"The time taken to train for mlp model =\", end2-start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7ec2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
